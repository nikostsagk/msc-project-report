%% ----------------------------------------------------------------
%% Thesis.tex
%% ---------------------------------------------------------------- 
\documentclass{ecsthesis}      	% Use the Thesis Style
\graphicspath{{../figures/}}   	% Location of your graphics files
\usepackage{natbib}            	% Use Natbib style for the refs.
\usepackage{xcolor}			% Footnotes on tables
\usepackage{footnote}		% Footnotes on tables
\usepackage[bottom]{footmisc} % Footnotes on the botom
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\hypersetup{colorlinks=true}   	% Set to false for black/white printing
\input{Definitions}            		% Include your abbreviations
%% ----------------------------------------------------------------
\begin{document}
\frontmatter
\title      {Improving Apple Detection and Counting Using RetinaNet}
\authors    {\texorpdfstring
             {\href{mailto:n.tsagko@gmail.com}{Nikolaos Chrysovalantis Tsagkopoulos}}
             {Nikolaos Chrysovalantis Tsagkopoulos}
            }
\addresses  {\groupname\\\deptname\\\univname}
\date       {\today}
\subject    {}
\keywords   {}
\maketitle
\begin{abstract}
This work aims to investigate the apple detection problem through the deployment of the RetinaNet object detection framework in conjunction with the VGG architecture. Following hyper-parameters' optimisation, the performance scaling with the backbone's network depth is examined through four different proposed deployments for the side-network. Analysis of the relationship between performance and training size establishes that 10 samples are enough to achieve adequate performance, while 200 samples are enough to achieve state-of-the-art performance. Moreover, a novel lightweight model is proposed that achieves an F1-score of 0.908 and inference time of nearly 70FPS. These results outperform previous state-of-the-art models in both performance and detection rates. Finally, the results are discussed regarding the model's limitations, and insights for future work are provided. 
\end{abstract}

\statementoforiginality{I have acknowledged all sources and identified any content taken from elsewhere. RetinaNet is based on the open-source implementation provided by Fizyr, licensed under the Apache License 2.0, and is appropriately cited. I did all the work myself and have not been helped by anyone else. The material in the report is genuine, and I have included all the necessary links with my data/code. I have not submitted any part of this work for another assessment. My work did not involve human participants, their cells or data, or animals.}
\tableofcontents
\listoffigures
\listoftables
%\lstlistoflistings
\listofsymbols{ll}{
$2D$ & two dimensional \\
$AUC$ & area under curve \\
$CCD$ & charge-coupled device \\
$CNN$ & convolutional neural network \\
$CRF$ & conditional random field \\
$FCN$ & fully convolutional network \\
$FN$ & false negative \\
$FP$ & false positive \\
$GMM$ & Gaussian mixture models \\
$GPS$ & global positioning system \\
$GPU$ & graphical processing unit \\
$HSV$ & Hue, Saturation and Value \\
$IoU$ & Intersection over Union \\
$mAP$ & mean average precision \\
$MLP$ & multi-layer perceptron \\
$NIR$ & near infra-red \\
$NMS$ & non-maximum suppression \\
$OHEM$ & online hard example mining \\
$PCA$ & principal component analysis \\
$ReLU$ & Rectified Linear Unit \\
$RGB$ & Red, Green and Blue \\
$RoI$ & Region of Interest \\
$RPN$ & region proposal network \\
$SGD$ & stochastic gradient descent \\
$SVM$ & support vector machine \\
$TN$ & true negative \\
$TP$ & true positive \\
$WS$ & watershed segmentation \\
}

%\acknowledgements{Thanks to no one.}
\dedicatory{\begin{flushright}"If I have seen further it is by standing on the shoulders of Giants." \\-Isaac Newton\end{flushright}}
\mainmatter
%% ----------------------------------------------------------------
\include{Introduction}
\include{ObjectDetection}
\include{RetinaNet}
\include{Experiment}
\include{Results}
\include{Conclusion}

%\appendix
%\include{AppendixA}
\backmatter
\bibliographystyle{ecs}
\bibliography{ECS}
\end{document}
%% ----------------------------------------------------------------
