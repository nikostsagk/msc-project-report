%% ----------------------------------------------------------------
%% Thesis.tex
%% ---------------------------------------------------------------- 
\documentclass{ecsthesis}      	% Use the Thesis Style
\graphicspath{{../figures/}}   	% Location of your graphics files
\usepackage{natbib}            	% Use Natbib style for the refs.
\usepackage{xcolor}			% Footnotes on tables
\usepackage{footnote}		% Footnotes on tables
\usepackage{xcolor}
\hypersetup{colorlinks=true}   	% Set to false for black/white printing
\input{Definitions}            		% Include your abbreviations
%% ----------------------------------------------------------------
\begin{document}
\frontmatter
\title      {Improving Apple Detection and Counting Using RetinaNet}
\authors    {\texorpdfstring
             {\href{mailto:n.tsagko@gmail.com}{Nikolaos Chrysovalantis Tsagkopoulos}}
             {Nikolaos Chrysovalantis Tsagkopoulos}
            }
\addresses  {\groupname\\\deptname\\\univname}
\date       {\today}
\subject    {}
\keywords   {}
\maketitle
\begin{abstract}
This work investigates the apple detection problem using the RetinaNet object detection framework and the VGG architecture. After all necessary modifications and hyper-parameter tuning, puts under examination how performance scales with the backbone's network depth. Furthermore, it examines performance through four different proposed deployments for the side-network. An analysis of the relation between performance and training size finds that 10 samples are enough to achieve adequate performance. It also discovers that 200 samples are enough to achieve state-of-the-art performance. Moreover, it proposes a novel lightweight model that achieves an F1-score of 0.908, and inference time of nearly 70FPS outperforming previous state-of-the-art models in both performance and detection rate. Finally, it discusses the results, addresses the model's limitations and weaknesses and provides insights for future work. 
\end{abstract}

\statementoforiginality{
I have acknowledged all sources, and identified any content taken from elsewhere. RetinaNet is based on the open-source implementation provided by Fizyr, licensed under the Apache License 2.0, and is appropriately cited. I did all the work myself and have not been helped by anyone else. The material in the report is genuine, and I have included all the necessary links with my data/code. I have not submitted any part of this work for another assessment. My work did not involve human participants, their cells or data, or animals.
}
\tableofcontents
\listoffigures
\listoftables
%\lstlistoflistings
\listofsymbols{ll}{
$2D$ & two dimensional \\
$AUC$ & area under curve \\
$CCD$ & charge-coupled device \\
$CNN$ & convolutional neural network \\
$CRF$ & conditional random field \\
$FCN$ & fully convolutional network \\
$FN$ & false negative \\
$FP$ & false positive \\
$GMM$ & Gaussian mixture models \\
$GPS$ & global positioning system \\
$GPU$ & graphical processing unit \\
$HSV$ & Hue, Saturation and Value \\
$IoU$ & Intersection over Union \\
$mAP$ & mean average precision \\
$MLP$ & multi-layer perceptron \\
$NIR$ & near infra-red \\
$NMS$ & non-maximum suppression \\
$PCA$ & principal component analysis \\
$ReLU$ & Rectified Linear Unit \\
$RGB$ & Red, Green and Blue \\
$RoI$ & Region of Interest \\
$RPN$ & region proposal network \\
$SGD$ & stochastic gradient descent \\
$SVM$ & support vector machine \\
$TN$ & true negative \\
$TP$ & true positive \\
$WS$ & watershed segmentation \\
}

%\acknowledgements{Thanks to no one.}
%\dedicatory{"If I have seen further it is by standing on the shoulders of Giants." -Isaac Newton}
\mainmatter
%% ----------------------------------------------------------------
\include{Introduction}
\include{ObjectDetection}
\include{Experiment}
\include{Results}
\include{Conclusion}

%\appendix
%\include{AppendixA}
\backmatter
\bibliographystyle{ecs}
\bibliography{ECS}
\end{document}
%% ----------------------------------------------------------------
